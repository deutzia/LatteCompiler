Iwona Kotlarska

Compilation is done using stack, which is wrapped around cabal. Used versions
of lexing and parsing tools were:
* stack 2.3.3
* cabal 3.0.0.0 compiled using version 3.0.1.0 of the Cabal library
* bnfc 2.8
* Alex version 3.2.5
* Happy Version 1.19.12

Autogenerated files are located in src/Parsing.

I made some assumptions about the exact semantics of the language that were
underspecified.

Returns: `return void` is allowed in a function returning void.
`return`, `while (true)` or `error` are instructions that are considered to be
sufficient to mark function as returning the correct type (even though in two
cases it never returns).

Classes: accessing fields and methods from methods of the class is possible both
without a modifier and using `self`.

Code generation assumptions: generated code is x86_64 assembly, which is then
assembled using nasm.

Even though int in Java has 32 bits, I decided to go for 64-bit ints. This
simplified code generation and I think it's reasonable, since my target
architecture is 64-bit anyway.

Calling convention is: all the arguments are on the stack, and the return value
is also returned on the stack, all of that space is allocated by caller.
Registers are not saved, because all the variables are on the stack anyway (I
plan to change it in the next iteration).

Due to different calling convention than in C, additional layer of translation
between latc generated code and linked C code is needed, therefore libs/ contain
not only the runtime.c with implementation of standard function, but also
calls.asm, which translates from one calling convention to another.

There is support for:
* arrays
* structures
* objects (fields as in structures, methods, inheritance)
* virtual methods

The assumption is that all methods are virtual, and at offset 0 in an object
there is a pointer to a vtable, and fields start from offset 1.

Optimizations supported: note that GCSE = LCSE performed on all blocks with the
same cache. However, since my code is not in SSA form - I write to variables
multiple times, but not to temporary registers. This can be, however, fixed in
quad generation - blocks of code have to be aware which variables have to be
refetched at the beginning. This way, GCSE is implemented, but in a more
inefficient way than it could've been - all variables that are fetched in
substatements are refetched.

Tail call optimization is also supported by changing call to a particular
function to jump to internal place of that function. This is slightly cursed,
because as a result every function has an additional functionName.internal
label after stack initialization.

Example tests of noteworthy behaviours are located in example_tests directory:
* test_arr_struct.lat shows that arrays of objects, objects with arrays etc are
  correctly supported
* test_gcse shows that expressions are not recomputed in a branch of an
  if-expression - blocks with labels other than lat_main are short and clearly
  don't perform complicated computations
* test_lcse_correctness.lat shows the correct behaviour in case of function calls
  since those can perform memory writes. As a result, all cached memory reads
  are invalidated after a function call or at the beginning of a new block
* tail_call.lat - loops forever on recursive calls, without tail call
  optimization this would throw a segfault due to stack overflow
* tco.lat - another example of tail call (with two arguments)
